---
title: Large Multimodal Model (LMM)
tags: ArtificialIntelligence, Multimodal, DeepLearning
---

# Large Multimodal Model (LMM)

The AI that has eyes and ears, not just a brain in a jar.

A **Large Multimodal Model (LMM)** is an AI system that can process and understand information from multiple "modalities" (data types) simultaneously; such as text, images, audio, and video.

Standard Large Language Models (LLMs) like early GPT-3 were text-only; they were like a brilliant philosopher locked in a dark room who could only read books. LMMs (like GPT-4o or Gemini) are like giving that philosopher eyes and ears. They can look at a photo and tell you what's funny about it, or listen to a song and describe the instruments.

## The Shift
*   **Uni-modal (Traditional AI):** Good at one thing (e.g., an AI that only classifies images of cats).
*   **Multi-modal (LMM):** Good at understanding the relationship *between* things. It knows that the *sound* of a bark, the *image* of a retriever, and the *word* "Dog" are all the same concept.

## FAQs

*1. Is GPT-4 an LMM?*
Yes. Specifically, versions like GPT-4o (omni) are native LMMs trained on text, audio, and images from the start, rather than just stitching separate models together.

*2. Why is it harder to build?*
Because aligning different senses is difficult. The AI has to learn that the cluster of pixels shaped like a circle and the concept of "Ball" are equivalent.

### Further Reading

*   **Concept:** *[What is Multimodal AI?](https://zapier.com/blog/multimodal-ai/)* (Zapier Guide).
*   **Survey:** *[A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549)* (Technical Deep Dive).
*   **DeepMind:** *[Gemini: A Family of Highly Capable Multimodal Models](https://deepmind.google/technologies/gemini/)* (Example of a native LMM).
